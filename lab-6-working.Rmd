---
title: "lab-7-working"
author: "Vienna Saccomanno"
date: "2/21/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tmap)
library(sf)
library(spatstat)
library(maptools)
library(sp)
library(raster)
# library(rgdal)
library(gstat)
library(plotKML) # for points to raster (they won't have this...just testing)
```

###Part 1. Hawaii raster intro

```{r}

# Read in the raster data because sf is only set up for vector data

hi_par <- raster("PAR_CLIM_M.tif")
hi_sst <- raster("SST_LTM.tif")
hi_chl <- raster("CHL_LTM.tif")
  
# Base plots
plot(hi_par)
plot(hi_sst)
plot(hi_chl)

par(mrow = c(1,3))
plot(hi_par)
plot(hi_sst)
plot(hi_chl)

#hi_sst - tells you about the data = raster

```

First: some useful functions for rasters

Checking it out: 

- crs
- reprojection
- cropping
- simple algebra example


```{r}

hi_sst@crs # Shows CRS(coordinate reference system): NAD83

hi_sst@extent # Shows extent (bounds)

```

Example: reprojection to WGS84
```{r}
wgs84 = "+proj=longlat +datum=WGS84 +ellps=WGS84 +no_defs" # Projection information, Just have this ready to copy/paste

# Reproject hi_sst to be WGS84. Biliner = liner interpertation in to directions
hi_sst_84 = projectRaster(hi_sst, crs = wgs84, method = "bilinear")

# Check the reprojection - looks good
hi_sst_84@crs
hi_sst_84@extent #now the lat long are in familiar units

```

raster::aggregate() for resampling to make files size smaller
```{r}

# Sea surface temperature: 
sst_rs <- aggregate(hi_sst, fact = 10) #Decreasing resolution by order of 10, Quick way to resample. Raster data files can get super massive and if you just want to test code, might want to resample to make things quicker.

plot(sst_rs)

# Plot side-by-side for comparison:
par(mfrow = c(1,2))
plot(hi_sst)
plot(sst_rs)

```

Crop a raster: 
```{r}

#what are the current extents?, get the bounding box for the raster
hi_sst_84@extent

# Get these extents from hi_sst_84 (call in console to see) what the actual limits are for hi_sst_84, then decide on cropping boundaries. Allison chose a Hawaiian island for this lab.

# First create a spatial polygon. as() forces this to be a spatial polygon.
bounds <- as(extent(-156.2, -154.5, 18.7, 20.4), 'SpatialPolygons') # Keep in mind, this could be any polygon shape (state outline, county outline, etc.) 'spatialPolygons', tells R to convert to spatial polygons. We're cropping to a rectangle, but we could have pulled in shapefile data and cropped accordingly.

# Reproject - if no projection this is the easy way to do this. Making crs of "bounds" match the crs of hi_sst_84. Need them to match in order to do math with them. Bounds = polygon, hi_sst_84= raster.
crs(bounds) <- crs(hi_sst_84)

# Then crop the raster by the polygon bounds 
sst_crop <- crop(hi_sst_84, bounds)

# And plot:
plot(sst_crop)
```


A simple algebra example/ raster math: 

Let's say we're creating a nonsensical variable called "tropicality", which is the sum of the PAR + SST + 2*ChlA. How can we create a layer for tropicality? How do we map?

First let's reprojeect PAR (radiation) and ChlA and get everything into the same CRS:

Use method = "bilinear" for continuous variables, "ngm" /nearest neighbor for categorical
```{r}

#wgs84 is stored from above and we can reuse it here. Bilinear is the default for continuous
hi_par_84 <- projectRaster(hi_par, crs = wgs84, method = "bilinear")

hi_chla_84 <- projectRaster(hi_chl, crs = wgs84, method = "bilinear")

# Now we have PAR, Chl-a, and SST all in the same CRS (WGS84) and can start doing some simple algebra. 
```
Plot them side-by-side:
```{r}
par(mfrow = c(1,3))
plot(hi_sst_84)
plot(hi_chla_84)
plot(hi_par_84)

#Checking out the scales of the three variables. This way we know the potentail ranges of adding our raster data together (~75)
```


Raster math is pretty straightforward: 
```{r}
#It says raster extents are different - not perfectly additive. Only intersections included. Sometimes will need to coerce raster data to be perfectly aligned.

#Raster math challenges: different extents/resultions/projections, and big datasets

trop <- hi_par_84 + hi_sst_84 + 2*hi_chla_84
plot(trop)
```


And we might want to plot these in tmap instead: 

Let's look at sea surface temperature. 
```{r}

islands <- read_sf(dsn = 'islands', layer = "Island_boundaries") %>% #file name + file type
  dplyr::select(Island) %>% #only select island name. Make sure to call dplyr to use select form dplyr
  st_simplify(dTolerance = 10) %>% 
  st_transform(crs = 4326) #need to transform to make sure same crs

plot(islands)

tmap_mode("plot") # or switch to tmap_mode("view")

tm_shape(hi_sst_84) + 
  tm_raster(title = "Mean Sea Surface Temperature") + #raster layer first
  tm_layout(bg.color = "navyblue", #change background color
            legend.position = c("left","bottom"),
            legend.text.color = "white", 
            legend.text.size = 0.5) +
  tm_shape(islands) +
  tm_fill("darkgreen")

# Or name it and export
sst_map <- tm_shape(hi_sst_84) +
  tm_raster(title = "Mean Sea Surface Temperature") +
  tm_layout(bg.color = "navyblue", 
            legend.position = c("left","bottom"),
            legend.text.color = "white", 
            legend.text.size = 0.5) +
  tm_shape(islands) +
  tm_fill("darkgreen")

#Saving - will show up in the working directory
tmap_save(sst_map, "sst.png", height=5)

```

What if only want to retain parts of a raster?
Example: Conditional rasters and masking

Let's say we have a sensitive species and we're trying to find suitable habitat/ a place they like. They like warm water (average temp >= 25.6 deg C) and PAR below 54.
```{r}
# Call in console. Currently don't have matching extents, columns nor rows, we need to update:
extent(hi_sst_84) <- extent(hi_par_84)
compareRaster(hi_sst_84, hi_par_84) #just says different number of columns, becuase we just set extents
# Check compareRaster...nope. Mismatching columns & rows is still a problem. 

# But we also need to make sure they have the same number of rows & columns. So make a raster from scratch to match hi_par_84 (pull info form the console)
cr <- raster(nrow = 822, 
             ncol = 1229, 
             xmn = -160.4365, 
             xmx = -154.5373, 
             ymn = 18.7309, 
             ymx = 22.44634)

#Now we have a raster that has the same number of columns and rows as par, now can resample hi_sst_84 to match cr. Continuous variable = bilinear interpolation to resample and fill in the old columns
sst_new <- resample(hi_sst_84, cr, method = "bilinear")

compareRaster(sst_new, hi_par_84) # TRUE! = they match
```

Create cropped versions of just Kauai
```{r}
# Created 'bounds_main' as earlier: 

bounds_main <- as(extent(-159.9, -159.2, 21.7, 22.3), 'SpatialPolygons') # Keep in mind, this could be any polygon shape (state outline, county outline, etc.)

# Reproject
crs(bounds_main) <- crs(sst_new)

par_kauai <- crop(hi_par_84, bounds_main) #croping the PAR raster to fit Kauii
sst_kauai <- crop(sst_new, bounds_main) #cropping SST raster to fit Kauii

# Check out PAR:
plot(par_kauai)

# Then SST:
plot(sst_kauai)

```


Subsetting and setting everything else to NA
Now we only want to isolate regions where the temperature >= 25.4 and PAR < 54.
```{r}
# Habitat
par_hab <- par_kauai # just makes a copy
par_hab[par_hab >= 54.0] <- NA #take PAR raster info and subset every PAR value above 54 to NA

plot(par_hab) #only hab less than 54 shows up!

sst_hab <- sst_kauai # also makes a copy
sst_hab[sst_hab < 25.6] <- NA #temps greater than or equal to 35.6

plot(sst_hab)

#where do these overlap? Using raster::mask
suit_hab <- mask(sst_hab, par_hab)
plot(suit_hab)

par(mfrow = c(1,2))
plot(par_hab)
plot(sst_hab)

```


###Part 2. Point pattern analysis

Get the spatial data (counties and red tree voles)
```{r}
voles <- read_sf(dsn = 'redtreevoledata', layer = "ds033") %>% 
  dplyr::select(COUNTY) %>% 
  filter(COUNTY == "HUM") %>% 
  st_transform(crs = 4326)

# plot(voles)

# Get Humboldt County outline
humboldt <- read_sf(dsn = 'redtreevoledata', layer = "california_county_shape_file") %>% 
  filter(NAME == "Humboldt") %>% 
  dplyr::select(NAME)

st_crs(humboldt) <- 4326 #assigning a coordinate system

# plot(humboldt)

# Plot them together: 
tm_shape(humboldt) +
  tm_fill() +
  tm_shape(voles) +
  tm_dots(size = 0.2)

# Or with ggplot2: 
ggplot() +
  geom_sf(data = humboldt) +
  geom_sf(data = voles) +

#Save in the working directory. Can specify units, width, height...    
ggsave("humvoles.png", 
       units = "in", 
       width = 4, 
       height = 6, 
       dpi = 300)

# Another example (with tiff...there's also jpeg, png, etc.)

# tiff("humvoles2.tiff", units = "in", width = 5, height = 5, res = 300)

ggplot() +
  geom_sf(data = humboldt, fill = "black") +
  geom_sf(data = voles, color = "red", alpha = 0.5)

# dev.off()


```


We want to explore point patterns in a few different ways. Quadrats. Distance-based methods (neighbor analysis using G-funcation and K-function). 

First we need to convert to 'ppp' and 'owin' - the points and windows, as used by maptools and spatstat (because sf is still catching up for raster and point pattern analysis stuff). So need to convert from SF to general to point.
```{r}

voles_sp <- as(voles,"Spatial")
voles_ppp <- as(voles_sp, "ppp") #for point patterns. Check class

humboldt_sp <- as(humboldt, "Spatial") #make it a spatial dataframe
humboldt_win <- as(humboldt_sp, "owin") #make it a spatial window (outer window)
#using View()see that lat and long are stored as x and y. Now we have pointes and a bounding window which ppp() wants

voles_pb <- ppp(voles_ppp$x, voles_ppp$y, window = humboldt_win)

plot(voles_pb) #both border and points (events) exist in the same place

#test for spatial evenness by finding the intensity of events in each region. Null = evenness  not csr.
vole_qt <- quadrat.test(voles_pb, nx = 5, ny = 10) #voles_pb + nx and ny are number of columns/rows for the rectangles created 

#Error okay becuase there are some regions with really small countrs (<5). Testing the null hypothersis of spatial evenness, although it can be called a test for csr. Chi squared test of CSR gives really small p -value = reject null. We conclude that these evenets do not reflect spatial evenness.

# Returns: VoleQT
# Chi-squared test of CSR using quadrat counts

# data:  VolePPP 
# X-squared = 425.94, df = 45, p-value < 2.2e-16
# alternative hypothesis: two.sided 
# Reject the null hypothesis of spatial evenness! But we still don't know if more clustered or more uniform...

plot(voles_pb)
plot(vole_qt, add = TRUE, cex = 0.4) #plot qt on top of the combined dataframe
#top left = how many actually exist
#bottom = above or below expected density
#can see that for regions close to whole, expected value = 5.5
#it does not look like these observations are evenly distributed. But the way that you break up an area can really change the results you get. Could look like evennes in a 4x4

```


Plot kernal densities for spatial data: 
```{r}

point_density <- density(voles_pb, sigma = 0.02) #sigma is bandwidth. SUper changes the density- must be careful with kernal density to show hotspots. Kernals are completely arbitrary. Must report bandwidth! Bandwidth is an actual measure in space, so ask yourself if sigma is logical
plot(point_density)

# Can you start viewing this in tmap? Yes, rasterize it(but don't foget that the "raster data" her is arbitrary based off of the bandwidth. TMap is happy with raster data: 
wgs84 = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
vole_raster <- raster(point_density, crs = wgs84)

# Then plot: 
tm_shape(vole_raster) +
  tm_raster(midpoint = NA, 
            palette = "Blues", 
            legend.show = FALSE)

```

Nearest neighbor (G-function). Relevant for disease modeling.
2 approaches:
- G-function only considerers the distance of each observation to its nearest neighbor
- K-function (or l-function): consideres how close all neighboring objects are to an event
```{r}

#make a sequence to calculate nearest neighbors, proporitons.
r <- seq(0,0.15, by = 0.005) #what works for this data. Sequence of distance overwhich I'm going to calculate observations with a nearest neighbor within the range. 

#envelope creates simulaitons of different functions we select.
gfunction <- envelope(voles_pb, fun = Gest, r = r, nsim = 20, nrank = 2) # Give actual data, give built in function (gest), give distances (r), Sig level of Monte Carlo = 0.04

#Plot what actual observation look like on top of simulation.
#Plot observed as a function of r to see the g-funtion curve for our observations. By 1.5 it is at 1
#Need to compare to csr scenario = #gfunction#theo. Run together. Observation line is "higher"than theoretical = more clustered than csr (increaed proportion). Supports visual inspection
plot(gfunction$obs ~ gfunction$r, type = "l", col = "black", lty = 11)
lines(gfunction$hi ~ gfunction$r, type = "l", col = "blue", lty = 8)
lines(gfunction$theo ~ gfunction$r, type = "l", col = "red", lty = 6)
lines(gfunction$lo ~ gfunction$r, type = "l", col = "green", lty = 4)

# Confirms, in combination with quadrat.test, clustered data!

```

Nearest Neighbor by Ripley's K (using L standardization)
- asking how close ALL neighbors are. 
- concentric circles

```{r}
r2 <- seq(0,0.5, by = 0.05) #new sequence of distances because need to look at much bigger space with all neighbors

lfunction <- envelope(voles_pb, fun = Lest, r = r2, nsim = 20, rank = 2, global = TRUE) #global means apply over the entire window of the study area

plot(lfunction$obs ~ lfunction$r, type = "l", col = "black", lty = 11)
lines(lfunction$hi ~ lfunction$r, type = "l", col = "blue", lty = 8)
lines(lfunction$theo ~ lfunction$r, type = "l", col = "red", lty = 6)
lines(lfunction$lo ~ lfunction$r, type = "l", col = "green", lty = 4)

```

Diggle-Cressie-Loosmore-Ford test of CSR - statistical test for csr
```{r}
#Null = csr
#Small p-value that yes, this is sig diff than complete spatial randomness
#remember, this is not immue to problems like, if you have a tone of events, likely to find significance.

DCLFTest <- dclf.test(voles_pb, nsim = 30, rank = 2) 
DCLFTest

```

###Part 3. Kansas rainfall kriging 
- spatial interpolation by Kriging
- rainfall in parts of kansas
```{r}
# Get Kansas rainfall data
ks_rain <- read_csv("KSRain2.csv") #just csv so will need to be converted to SF data. R doesn't know that this is spatial
ks_sf  <-  st_as_sf(ks_rain, coords = c("LON", "LAT"),# making it SF object, call the csv, tell R where the coords are (lon first, then lat, finally give it a coord ref system)
                 crs = 4326)

plot(ks_sf) #we'll be using the amount 

# Get county data
ks_counties <- read_sf(dsn = 'KSCounties', layer = "ks_counties_shapefile") #this is SF, in the KSCounties folder, call layer
st_crs(ks_counties) = 4326 #doesn't have a projection so need to assign one.

# Plot with tmap:
tm_shape(ks_counties) +
  tm_fill() +
  tm_shape(ks_sf) + #add rain points
  tm_dots("AMT", size = 0.5) #select which attribute you want (we want "amount of rain")

#many obs, kinda evenly spread, some spots where no monitoring
#Problems in the area where minimal observations to make predictions with.

# Or with ggplot:
ggplot() +
  geom_sf(data = ks_counties, 
          fill = "gray10", 
          color = "gray20") +
  geom_sf(data = ks_sf, aes(color = AMT)) +
  scale_color_gradient(low = "yellow", 
                       high = "red") +
  theme_minimal() +
  coord_sf(datum = NA)

```

But we want to make predictions across the entire state using kriging. 

First, make the rainfall data a Spatial Points data frame: 
```{r}
ks_sp  <- as_Spatial(ks_sf) #spatial points data fram
```

Then make a spatial grid that we'll krige over (interpolate values over):
```{r}
# bbox(ks_sp) to check bounding box of the spatial points
#request resolution. Allison selected a grid that makes sense for kansas
lat <- seq(37, 40, length.out = 200)
long <- seq(-94.6,-102, length.out = 200)

# Take the two vectores and make r  make it into a grid: 
grid <- expand.grid(lon = long, lat = lat)
grid_sf <- st_as_sf(grid, coords = c("lon","lat"), crs = 4326) #convert to an SF object = a spatial grid with lat/lon/and projection
grid_sp <- as_Spatial(grid_sf) #need to convert back to spatial values datafram = spatial points data fram so that the stats package can read it

```


Then make a variogram and find the varigram model
```{r}

# Create the variogram:
ks_vgm <- variogram(AMT ~ 1, ks_sp) # The ~1 is the type of kriging = ordinary

# Look at it - telling us about varaiance between obser with distance. With rainfall, it makes sense that there will be some weighting with distance. Varigram tells about hwwighting should decay and given us a function.
plot(ks_vgm)

# Fit the variogram model using reasonable estimates for nugget, sill and range:
ks_vgm_fit <- fit.variogram(ks_vgm, model = vgm(nugget = 0.2, psill = 0.8, model = "Sph", range = 200))

# Plot them both together
plot(ks_vgm, ks_vgm_fit) # Cool! So what are the values

# Just FYI: there are other models (Gaussian, Exponential) - how do those line up? 
ks_vgm_gau <- fit.variogram(ks_vgm, model = vgm(nugget = 0.2, psill = 0.8, model = "Gau", range = 200))

plot(ks_vgm, ks_vgm_gau)

# You can check the sum of squares of residuals for each: 
attr(ks_vgm_fit, 'SSErr') # 0.00214 (and could compare to other models...)

# We'll stick with the Spherical model: 
ks_vgm_fit # Nugget = 0.102, sill = 0.954, range = 235
```

Now, kriging/ spatial interpolation! 
```{r}

ks_krige <- krige(AMT ~ 1, ks_sp, grid_sp, model=ks_vgm_fit) #krig function is from gstat. Actual data, grid to make new predictions for, and variogram

```

And visualize it: 
```{r}

ks_krige_df <- as.data.frame(ks_krige) # View it after this to show output

# Rename things to make it a little nicer
ks_krige_2 <- ks_krige_df %>% 
  rename(lon = coords.x1, lat = coords.x2, predicted = var1.pred, err = var1.var)

# Make this into spatial data again - make an SF object
rain_predicted  <-  st_as_sf(ks_krige_2, coords = c("lon", "lat"), 
                 crs = 4326)

# Get Kansas outline to crop to it - its in the folder called "states". 
ks <- read_sf(dsn = "states", layer = "cb_2017_us_state_20m") %>% 
  dplyr::select(NAME) %>% 
  filter(NAME == "Kansas") %>% 
  st_transform(crs = 4326)

# Crop the rainfall data gris by the kansas polygon
rain_cropped <- st_intersection(rain_predicted, ks)

# Initial plot
plot(rain_cropped) # But this is points

# So is this (cheating...)
# tmap: 
  tm_shape(rain_cropped) +
  tm_dots("predicted", size = 0.05) +
  tm_shape(ks_counties) +
  tm_borders() +
    tm_layout(legend.bg.color = "white", legend.position = c("left","bottom"))

```

Extra...converting sf points to Spatial points to raster (with plotKML package): 

```{r}

# Convert sf object to spatial points
rain_pts <- as_Spatial(rain_cropped)
class(rain_pts)

# Rasterize spatial points, make class 'Raster'
rain_raster <- vect2rast(rain_pts)
rain_raster2 <- raster(rain_raster)

# Need to aggregate so it's not just tiny cells (and white space)
rain_raster_agg <- raster::aggregate(rain_raster2, fact = 5, fun = max)

# Then plot the raster
tm_shape(rain_raster_agg) +
  tm_raster() +
  tm_shape(ks_counties) +
  tm_borders() +
    tm_layout(legend.bg.color = "white", legend.position = c("left","bottom"))



```

